{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VR9Kq2V4N66"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import csv\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "asciified_data = pd.read_csv('/content/drive/MyDrive/train_ascii.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "# Merge the datasets based on ID\n",
        "merged_data = pd.merge(asciified_data, train_data, on='ID')"
      ],
      "metadata": {
        "id": "5K10dtmE4TNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing"
      ],
      "metadata": {
        "id": "NcTJurdq4vEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize each letter in a sentence\n",
        "def letter_tokenization(sentence):\n",
        "    return list(sentence)\n",
        "\n",
        "# Apply letter tokenization to each sentence in both columns\n",
        "merged_data['Tokenized_x'] = merged_data['Sentence_x'].apply(letter_tokenization)\n",
        "merged_data['Tokenized_y'] = merged_data['Sentence_y'].apply(letter_tokenization)\n",
        "test_data['Tokenized'] = test_data['Sentence'].apply(letter_tokenization)"
      ],
      "metadata": {
        "id": "yBRaMqgJ4U5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create vocabulary\n",
        "char_to_index = {}\n",
        "index_to_char = {}\n",
        "\n",
        "# Add special tokens\n",
        "char_to_index['<PAD>'] = 0\n",
        "index_to_char[0] = '<PAD>'\n",
        "char_to_index['<UNK>'] = 1\n",
        "index_to_char[1] = '<UNK>'\n",
        "char_to_index['<EOS>'] = 2\n",
        "index_to_char[2] = '<EOS>'\n",
        "\n",
        "# Function to add words to vocabulary\n",
        "def add_to_vocab(chars):\n",
        "    for char in chars:\n",
        "        if char not in char_to_index:\n",
        "            char_to_index[char] = len(char_to_index)\n",
        "            index_to_char[len(char_to_index) - 1] = char\n",
        "\n",
        "# Create vocabulary from tokenized input and labels\n",
        "merged_data['Tokenized_x'].apply(add_to_vocab)\n",
        "merged_data['Tokenized_y'].apply(add_to_vocab)\n",
        "test_data['Tokenized'].apply(add_to_vocab)\n",
        "\n",
        "# Add <EOS> token to the end of each sentence\n",
        "merged_data['Tokenized_x'] = merged_data['Tokenized_x'].apply(lambda x: x + ['<EOS>'])\n",
        "merged_data['Tokenized_y'] = merged_data['Tokenized_y'].apply(lambda x: x + ['<EOS>'])\n",
        "test_data['Tokenized'] = test_data['Tokenized'].apply(lambda x: x + ['<EOS>'])"
      ],
      "metadata": {
        "id": "kEySTvsp4XXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to indices\n",
        "def tokens_to_indices(tokens):\n",
        "    return [char_to_index[char] for char in tokens]\n",
        "\n",
        "# Apply tokenization and indexing to the DataFrame\n",
        "merged_data['Indexed_x'] = merged_data['Tokenized_x'].apply(tokens_to_indices)\n",
        "merged_data['Indexed_y'] = merged_data['Tokenized_y'].apply(tokens_to_indices)"
      ],
      "metadata": {
        "id": "X-El6qWW4ZIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "1dc1Z4jh4smu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(char_to_index)\n",
        "embedding_dim = 100\n",
        "hidden_units = 64\n",
        "\n",
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
        "    Bidirectional(LSTM(units=hidden_units, return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(units=hidden_units, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    Dense(units=vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "RJDT5q964a6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_length = 1200  # train ~ 1800, test ~ 1100\n",
        "padded_input = pad_sequences(merged_data['Indexed_x'], maxlen=max_length, padding='post')\n",
        "padded_label = pad_sequences(merged_data['Indexed_y'], maxlen=max_length, padding='post')\n",
        "\n",
        "# Convert to numpy arrays\n",
        "padded_input = np.array(padded_input)\n",
        "padded_label = np.array(padded_label)"
      ],
      "metadata": {
        "id": "GyDKSfNi4fH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(padded_input, padded_label, epochs=5, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "h0uYuu5D4gnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predictions"
      ],
      "metadata": {
        "id": "0nik-Y0q4pMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on a subset of the training set (first 2 examples)\n",
        "train_predictions = model.predict(padded_input[0:4])\n",
        "# Display examples\n",
        "for idx in range(4):\n",
        "    print(\"Example\", idx+1)\n",
        "    print(\"Input:\", ' '.join([index_to_char[i] for i in padded_input[idx] if i != 0]))  # Remove padding\n",
        "    print(\"Label:\", ' '.join([index_to_char[i] for i in padded_label[idx] if i != 0]))  # Remove padding\n",
        "    # Get predicted indices for the current example\n",
        "    predicted_indices = train_predictions[idx].argmax(axis=1)\n",
        "    # Remove padding and stop at <EOS> token\n",
        "    predicted_sentence = []\n",
        "    for i in predicted_indices:\n",
        "        if i == 0:  # Stop at padding\n",
        "            break\n",
        "        if index_to_char[i] == '<EOS>':  # Stop at <EOS>\n",
        "            break\n",
        "        predicted_sentence.append(index_to_char[i])\n",
        "    print(\"Prediction:\", ''.join(predicted_sentence))\n",
        "    print()"
      ],
      "metadata": {
        "id": "qWOO3Tf74in0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test dataset\n",
        "test_data = pd.read_csv('/content/drive/My Drive/test.csv')\n",
        "\n",
        "test_data['Sentence'] = test_data['Sentence'].apply(remove_first_char)\n",
        "\n",
        "test_data['Sentence'] = test_data['Sentence'].str.lower()\n",
        "\n",
        "# Tokenize test sentences\n",
        "test_data['tokenized_input'] = test_data['Sentence'].apply(letter_tokenization)\n",
        "\n",
        "test_data['tokenized_input'] = test_data['tokenized_input'].apply(lambda x: x + ['<EOS>'])\n",
        "\n",
        "# Convert test sentences to indices\n",
        "test_data['indexed_input'] = test_data['tokenized_input'].apply(tokens_to_indices)\n",
        "\n",
        "# Pad test sequences\n",
        "padded_test_input = pad_sequences(test_data['indexed_input'], maxlen=max_length, padding='post')\n",
        "\n",
        "# Predict labels for test data\n",
        "test_predictions = model.predict(padded_test_input)\n",
        "\n",
        "# Convert predicted indices to sentences\n",
        "predicted_sentences = []\n",
        "for prediction in test_predictions:\n",
        "    predicted_sentence = []\n",
        "    for i in prediction.argmax(axis=1):\n",
        "        if i == 0:  # Stop at padding\n",
        "            break\n",
        "        if index_to_char[i] == '<EOS>':  # Stop at <EOS>\n",
        "            break\n",
        "        predicted_sentence.append(index_to_char[i])\n",
        "    predicted_sentences.append(''.join(predicted_sentence))\n",
        "\n",
        "# Add predicted sentences to test_data\n",
        "test_data['Predicted_Sentence'] = predicted_sentences"
      ],
      "metadata": {
        "id": "eXWJ5W5V4kpO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}